#+TITLE:MARVELpipeline
Data processing and radial velocity estimation pipeline of the MARVEL spectrograph


** Running the different components of the pipeline

The pipeline is made out of different components that can run one after the other.
Every time a component finishes it will create a FITS file with the intermediate results
saved into it. This file can then serve as input for another component. An example of
how these components rely on the results of previous components is illustrated in the following graph:

#+CAPTION:Shows components needed to create an "Optimal Science Extraction" image and how these are in turn dependent on other components.
#+NAME: fig:Optimal Extraction
[[./Docs/Images/my_output_file.png]]

** Dependencies

*** Rust and dvc

The pipeline components are written in python or [[https://foundation.rust-lang.org/][rust]]. To manage the pipeline we use a command
tool called [[https://dvc.org/][dvc]]. In the following section we explain how to install the python packages using
poetry. To install [[https://www.rust-lang.org/tools/install][rust]] and [[https://dvc.org/#get-started-dvc][dvc]] we recomend following the official installation guide given on
their respective websites. 

*** Python dependencies

Before starting installing the packages, we recomend using a software tool to manage different environments
like [[https://docs.conda.io/projects/conda/en/stable/commands/create.html][conda]] to create a environment for marvel. Using conda, use the command:

#+begin_src shell
  conda create -n marvelpipeline [python=<python version>]
#+end_src

where the ~<python-version>~ should be higher than python 3.8. We can then activate the environment with

#+begin_src shell
  conda activate marvelpipeline
#+end_src

To install the needed python packages we recomend installing [[https://python-poetry.org/][poetry]] using [[https://python-poetry.org/docs/][the official installation]].
This tool allows us to install all the packages with one command in a deterministic way.
After installation, we can check that poetry was successfully installed by running

#+begin_src shell
  poetry --version
#+end_src

If we receive an error something went wrong when installing poetry. If not we can install the dependencies
with one command:

#+begin_src shell
  poetry install
#+end_src


** Compiling the pipeline

Part of the pipeline is written using Rust. This code needs to be compile before it is able to be ran.
This process a made easy by simply running the command:

#+begin_src shell
  ./compile
#+end_src


** Configuration

Before the pipeline is able to be run, it needs to know which files it can use as input. This input is provided by a simple
input YAML file (~inputfile.yaml~). The YAML file consists simply of three keys _rawBiasImage_, _rawFlatImage_ and _rawScienceImage_. Their respective
values are lists containing the corresponding files that should be used as input for the pipeline. An example of such an imput file
is given in ~inputfile.yaml~.

*** The =configure.py= script

When the pipeline is ran, multiple intermediate files are created in the process. This is to help troubleshoot if we want to
check certain calculation. For the pipeline to know where these files can be saved and do not overwrite other files these
paths are set up in a ~params.yaml~ file. This file can be generated from the ~inputfile.yaml~ file using the ~configure.py~ script
as:

#+begin_src shell
  python configure.py inputfile.yaml
#+end_src

This way we get a sensible input file ~params.yaml~ using a much more minimal file ~inputfile.yaml~.


** Running the entire pipeline

Finaly the pipeline can be run using dvc by running

#+begin_src shell
  dvc repro
#+end_src








** Running an individual component

*** Initializing a component

For the components to be created we will need (at least) two kinds of input arguments.

The first input argument is the database that will be used. This can be a =DatabaseFromLocalFiles= object
in which case an ascii file will be used to keep track of all pipeline input/output files, or it can be None,
in which case MongoDB will be used as a database.

The second argument is a keyword argument Collection=[list of FITS files] where Collection
is one of the collection names in the left column in the table below. The right-hand column shows what
type of data products they can be given as values. The list of FITS files can be either a list of file paths
(strings) relative to the base dir of this project, or a list of unique identification hashes (strings).

| Collection       | Files                                                                    |
|------------------+--------------------------------------------------------------------------|
| BiasImages       | Raw Bias Images, Master Bias Images                                      |
| Flat Images      | Raw Flat Images, Master Flat Images                                      |
| EtalonImages     | Raw Etalon Images, Calibrated Etalon Images                              |
| ScienceImages    | Raw Science Images, Calibrated Science Images                            |
| ExtractedOrders  | Extracted Etalon Orders, Extracted Science Orders, Extracted Flat Orders |
| OptimalExtracted | Optimal Extracted Science, Optimal Extracted Etalon                      |


In some cases, there is an optional argument that will signify the debug mode when running the component.
The debug option can be an int ranging from 0 to 3, where 0 means no debug output and 3 means lots of debug output.

We illustrate how to run a component with an example. From the previous image, we can see that in order to obtain
a *Science Order Extraction* component, we need a *Calibrated Science* and *Order Mask Extraction* as input.

#+begin_src python
  from database        import DatabaseFromLocalFiles
  from orderExtraction import OrderExtraction

  # We run a DatabaseFromLocalFiles object.
  # Since by default such an object only keeps track of raw images, we need to load in a previously generated txt file
  # that also contains a "Calibrated Science Image" and an "Extracted Flat Image".

  db = DatabaseFromLocalFiles("input_file_name.txt")

  # We identify that "Calibrated Science Image" with a hash

  scienceImageHash = "b0ef6a99bde7cdbc968a46fcd7a57e450a554c548d9cc89d7a9555e7236fe05f"

  # and we identify the "Extracted Flat Image" with a path (relative to the MARVELpipeline base dir)

  orderMaskPath = "Data/ProcessedData/ExtractedOrders/orderMask.fits"

  # The component can then be constructed by running

  scienceExtractor = OrderExtraction(db, ExtractedOrders=orderMaskPath, ScienceImages=scienceImageHash, debug=1)
#+end_src

*** Running a component

After having initialized the component, we can run the component by calling the =run()= method.
This method takes an optional argument =outputFileName=. If no argument is given no output file
is saved, otherwise a file with name =outputFileName= is generated as a final product of having
ran the component and the product is added to the database.

#+begin_src python

  # run the component and save the outputfile as extractedScienceTestF.fits

  scienceExtractor1.run("extractedScienceTestF.fits")

  # After having added the "Extracted Science Orders" to the database we want
  # be able to call this file later by saving the new database to an output file

  db.save()

#+end_src


*** Running the whole pipeline

It is usefull to keep track of all the files that are needed to run every the component.
The whole pipeline is given here to give in a schematic overview of the currently implemented pipeline.

#+CAPTION:Overview of entire pipeline
#+NAME: fig:whole_pipeline
[[./Docs/Images/whole_pipeline_file.png]]

** Running the pipeline without MongoDB

Normally the pipeline should work in conjunction with a database that keeps track of all the
files and metadata that is generated during the different steps in the pipeline. This is done
either with a MongoDB server (a cross-platform document-oriented NoSQL database program)
running at the observatory or, if you want to do local tests, with a database based on a local
ascii file. The latter can be instantiated and used, for example by:

#+begin_src python
  from database import DatabaseFromLocalFiles

  # Create a DatabaseFromLocalFiles object
  db = DatabaseFromLocalFiles("database_object_file.txt")

  ...

  # After running the pipeline we might want to keep track of the files that were created.
  # This can be done by saving the database. This will save a text file that can be used to
  # initialize another database object containing the same information.

  db.save()
#+end_src


